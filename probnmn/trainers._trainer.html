
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <meta charset="utf-8" />

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-120523111-2"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-120523111-2');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Inconsolata&display=swap" rel="stylesheet">


    <title>probnmn.trainers._trainer &#8212; ProbNMN 1.0.0 documentation</title>

    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/language_data.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="probnmn.trainers.program_prior_trainer" href="trainers.program_prior_trainer.html" />
    <link rel="prev" title="probnmn.trainers" href="trainers.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="module-probnmn.trainers._trainer">
<span id="probnmn-trainers-trainer"></span><h1>probnmn.trainers._trainer<a class="headerlink" href="#module-probnmn.trainers._trainer" title="Permalink to this headline">¶</a></h1>
<dl class="class">
<dt id="probnmn.trainers._trainer._Trainer">
<em class="property">class </em><code class="descclassname">probnmn.trainers._trainer.</code><code class="descname">_Trainer</code><span class="sig-paren">(</span><em>config: probnmn.config.Config, dataloader: torch.utils.data.dataloader.DataLoader, models: Dict[str, torch.nn.modules.module.Module], serialization_dir: str, gpu_ids: List[int] = [0]</em><span class="sig-paren">)</span><a class="reference external" href="http://github.com/kdexd/probnmn-clevr/blob/master/probnmn/trainers/_trainer.py#L12-L299"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#probnmn.trainers._trainer._Trainer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>A base class for generic training of models. This class can have multiple models interacting
with each other, rather than a single model, which is suitable to our use-case (for example,
<code class="docutils literal notranslate"><span class="pre">module_training</span></code> phase has two models:
<a class="reference internal" href="models.program_generator.html#probnmn.models.program_generator.ProgramGenerator" title="probnmn.models.program_generator.ProgramGenerator"><code class="xref py py-class docutils literal notranslate"><span class="pre">ProgramGenerator</span></code></a> and
<a class="reference internal" href="models.nmn.html#probnmn.models.nmn.NeuralModuleNetwork" title="probnmn.models.nmn.NeuralModuleNetwork"><code class="xref py py-class docutils literal notranslate"><span class="pre">NeuralModuleNetwork</span></code></a>). It offers full flexibility, with sensible
defaults which may be changed (or disabled) while extending this class.</p>
<ol class="arabic simple">
<li><p>Default <a class="reference external" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Adam" title="(in PyTorch vmaster (1.1.0 ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">Adam</span></code></a> Optimizer, updates parameters of all models in this
trainer. Learning rate and weight decay for this optimizer are picked up from the provided
config.</p></li>
<li><p>Default <a class="reference external" href="https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.ReduceLROnPlateau" title="(in PyTorch vmaster (1.1.0 ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReduceLROnPlateau</span></code></a> learning rate scheduler. Gamma
and patience arguments are picked up from the provided config. Observed metric is assumed
to be of type “higher is better”. For ‘lower is better” metrics, make sure to reciprocate.</p></li>
<li><p>Tensorboard logging of loss curves, metrics etc.</p></li>
<li><p>Serialization of models and optimizer as checkpoint (.pth) files after every validation.
The observed metric for keeping track of best checkpoint is of type “higher is better”,
follow (2) above if the observed metric is of type “lower is better”.</p></li>
</ol>
<p>Extend this class and override suitable methods as per requirements, some important ones are:</p>
<ol class="arabic">
<li><p><a class="reference internal" href="#probnmn.trainers._trainer._Trainer.step" title="probnmn.trainers._trainer._Trainer.step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">step()</span></code></a>, provides complete customization, this is the method which comprises of one
full training iteration, and internally calls (in order) - <a class="reference internal" href="#probnmn.trainers._trainer._Trainer._before_iteration" title="probnmn.trainers._trainer._Trainer._before_iteration"><code class="xref py py-meth docutils literal notranslate"><span class="pre">_before_iteration()</span></code></a>,
<a class="reference internal" href="#probnmn.trainers._trainer._Trainer._do_iteration" title="probnmn.trainers._trainer._Trainer._do_iteration"><code class="xref py py-meth docutils literal notranslate"><span class="pre">_do_iteration()</span></code></a> and <a class="reference internal" href="#probnmn.trainers._trainer._Trainer._after_iteration" title="probnmn.trainers._trainer._Trainer._after_iteration"><code class="xref py py-meth docutils literal notranslate"><span class="pre">_after_iteration()</span></code></a>. Most of the times you may not require
overriding this method, instead one of the mentioned three methods called by <cite>:meth:`step</cite>.</p></li>
<li><p><a class="reference internal" href="#probnmn.trainers._trainer._Trainer._do_iteration" title="probnmn.trainers._trainer._Trainer._do_iteration"><code class="xref py py-meth docutils literal notranslate"><span class="pre">_do_iteration()</span></code></a>, with core training loop - what happens every iteration, given a
<code class="docutils literal notranslate"><span class="pre">batch</span></code> from the dataloader this class holds.</p></li>
<li><p><a class="reference internal" href="#probnmn.trainers._trainer._Trainer._before_iteration" title="probnmn.trainers._trainer._Trainer._before_iteration"><code class="xref py py-meth docutils literal notranslate"><span class="pre">_before_iteration()</span></code></a> and <a class="reference internal" href="#probnmn.trainers._trainer._Trainer._after_iteration" title="probnmn.trainers._trainer._Trainer._after_iteration"><code class="xref py py-meth docutils literal notranslate"><span class="pre">_after_iteration()</span></code></a>, for any pre- or post-processing
steps. Default behaviour:</p>
<blockquote>
<div><ul class="simple">
<li><p><a class="reference internal" href="#probnmn.trainers._trainer._Trainer._before_iteration" title="probnmn.trainers._trainer._Trainer._before_iteration"><code class="xref py py-meth docutils literal notranslate"><span class="pre">_before_iteration()</span></code></a> - call <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code></p></li>
<li><p><a class="reference internal" href="#probnmn.trainers._trainer._Trainer._after_iteration" title="probnmn.trainers._trainer._Trainer._after_iteration"><code class="xref py py-meth docutils literal notranslate"><span class="pre">_after_iteration()</span></code></a> - call <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code> and do tensorboard logging.</p></li>
</ul>
</div></blockquote>
</li>
<li><p><a class="reference internal" href="#probnmn.trainers._trainer._Trainer.after_validation" title="probnmn.trainers._trainer._Trainer.after_validation"><code class="xref py py-meth docutils literal notranslate"><span class="pre">after_validation()</span></code></a>, to specify any steps after evaluation. Default behaviour is to
do learning rate scheduling and log validation metrics on tensorboard.</p></li>
</ol>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>config: Config</strong></dt><dd><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">Config</span></code> object with all the relevant configuration parameters.</p>
</dd>
<dt><strong>dataloader: torch.utils.data.DataLoader</strong></dt><dd><p>A <a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" title="(in PyTorch vmaster (1.1.0 ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code></a> which provides batches of training examples. It
wraps one of <a class="reference internal" href="data.datasets.html#module-probnmn.data.datasets" title="probnmn.data.datasets"><code class="xref py py-mod docutils literal notranslate"><span class="pre">probnmn.data.datasets</span></code></a> depending on the evaluation phase.</p>
</dd>
<dt><strong>models: Dict[str, Type[nn.Module]]</strong></dt><dd><p>All the models which interact with each other during training. These are one or more from
<code class="xref py py-mod docutils literal notranslate"><span class="pre">probnmn.models</span></code> depending on the training phase.</p>
</dd>
<dt><strong>serialization_dir: str</strong></dt><dd><p>Path to a directory for tensorboard logging and serializing checkpoints.</p>
</dd>
<dt><strong>gpu_ids: List[int], optional (default=[0])</strong></dt><dd><p>List of GPU IDs to use or evaluation, <code class="docutils literal notranslate"><span class="pre">[-1]</span></code> - use CPU.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>All models are <cite>passed by assignment</cite>, so they could be shared with an external evaluator.
Do not set <code class="docutils literal notranslate"><span class="pre">self._models</span> <span class="pre">=</span> <span class="pre">...</span></code> anywhere while extending this class.</p>
<dl class="method">
<dt id="probnmn.trainers._trainer._Trainer.step">
<code class="descname">step</code><span class="sig-paren">(</span><em>iteration: Optional[int] = None</em><span class="sig-paren">)</span><a class="reference external" href="http://github.com/kdexd/probnmn-clevr/blob/master/probnmn/trainers/_trainer.py#L134-L150"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#probnmn.trainers._trainer._Trainer.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform one iteration of training.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>iteration: int, optional (default = None)</strong></dt><dd><p>Iteration number (useful to hard set to any number when loading checkpoint).
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, use the internal <code class="xref py py-attr docutils literal notranslate"><span class="pre">self._iteration</span></code> counter.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="probnmn.trainers._trainer._Trainer._before_iteration">
<code class="descname">_before_iteration</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="http://github.com/kdexd/probnmn-clevr/blob/master/probnmn/trainers/_trainer.py#L152-L157"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#probnmn.trainers._trainer._Trainer._before_iteration" title="Permalink to this definition">¶</a></dt>
<dd><p>Steps to do before doing the forward pass of iteration. Default behavior is to simply
call <code class="xref py py-meth docutils literal notranslate"><span class="pre">zero_grad()</span></code> for optimizer. Called inside <a class="reference internal" href="#probnmn.trainers._trainer._Trainer.step" title="probnmn.trainers._trainer._Trainer.step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">step()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="probnmn.trainers._trainer._Trainer._do_iteration">
<code class="descname">_do_iteration</code><span class="sig-paren">(</span><em>batch: Dict[str, Any]</em><span class="sig-paren">)</span> &#x2192; Dict[str, Any]<a class="reference external" href="http://github.com/kdexd/probnmn-clevr/blob/master/probnmn/trainers/_trainer.py#L159-L179"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#probnmn.trainers._trainer._Trainer._do_iteration" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward and backward passes on models, given a batch sampled from dataloader.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>batch: Dict[str, Any]</strong></dt><dd><p>A batch of training examples sampled from dataloader. See <a class="reference internal" href="#probnmn.trainers._trainer._Trainer.step" title="probnmn.trainers._trainer._Trainer.step"><code class="xref py py-func docutils literal notranslate"><span class="pre">step()</span></code></a> and
<a class="reference internal" href="#probnmn.trainers._trainer._Trainer._cycle" title="probnmn.trainers._trainer._Trainer._cycle"><code class="xref py py-meth docutils literal notranslate"><span class="pre">_cycle()</span></code></a> on how this batch is sampled.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>Dict[str, Any]</strong></dt><dd><p>An output dictionary typically returned by the models. This would be passed to
<a class="reference internal" href="#probnmn.trainers._trainer._Trainer._after_iteration" title="probnmn.trainers._trainer._Trainer._after_iteration"><code class="xref py py-meth docutils literal notranslate"><span class="pre">_after_iteration()</span></code></a> for tensorboard logging.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="probnmn.trainers._trainer._Trainer._after_iteration">
<code class="descname">_after_iteration</code><span class="sig-paren">(</span><em>output_dict: Dict[str, Any]</em><span class="sig-paren">)</span><a class="reference external" href="http://github.com/kdexd/probnmn-clevr/blob/master/probnmn/trainers/_trainer.py#L181-L205"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#probnmn.trainers._trainer._Trainer._after_iteration" title="Permalink to this definition">¶</a></dt>
<dd><p>Steps to do after doing the forward pass of iteration. Default behavior is to simply
do gradient update through <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code>, and log metrics to tensorboard.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>output_dict: Dict[str, Any]</strong></dt><dd><p>This is exactly the object returned by :meth:_do_iteration`, which would contain all
the required losses for tensorboard logging.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="probnmn.trainers._trainer._Trainer.after_validation">
<code class="descname">after_validation</code><span class="sig-paren">(</span><em>val_metrics: Dict[str, Any], iteration: Optional[int] = None</em><span class="sig-paren">)</span><a class="reference external" href="http://github.com/kdexd/probnmn-clevr/blob/master/probnmn/trainers/_trainer.py#L207-L249"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#probnmn.trainers._trainer._Trainer.after_validation" title="Permalink to this definition">¶</a></dt>
<dd><p>Steps to do after an external <a class="reference internal" href="evaluators._evaluator.html#probnmn.evaluators._evaluator._Evaluator" title="probnmn.evaluators._evaluator._Evaluator"><code class="xref py py-class docutils literal notranslate"><span class="pre">_Evaluator</span></code></a> performs
evaluation. This is not called by <a class="reference internal" href="#probnmn.trainers._trainer._Trainer.step" title="probnmn.trainers._trainer._Trainer.step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">step()</span></code></a>, call it from outside at appropriate time.
Default behavior is to perform learning rate scheduling, serializaing checkpoint and to
log validation metrics to tensorboard.</p>
<p>Since this implementation assumes a key <code class="docutils literal notranslate"><span class="pre">&quot;metric&quot;</span></code> in <code class="docutils literal notranslate"><span class="pre">val_metrics</span></code>, it is convenient
to set this key while overriding this method, when there are multiple models and multiple
metrics and there is one metric which decides best checkpoint.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>val_metrics: Dict[str, Any]</strong></dt><dd><p>Validation metrics for all the models. Returned by <code class="docutils literal notranslate"><span class="pre">evaluate</span></code> method of
<a class="reference internal" href="evaluators._evaluator.html#probnmn.evaluators._evaluator._Evaluator" title="probnmn.evaluators._evaluator._Evaluator"><code class="xref py py-class docutils literal notranslate"><span class="pre">_Evaluator</span></code></a> (or its extended class).</p>
</dd>
<dt><strong>iteration: int, optional (default = None)</strong></dt><dd><p>Iteration number. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, use the internal <code class="xref py py-attr docutils literal notranslate"><span class="pre">self._iteration</span></code> counter.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="probnmn.trainers._trainer._Trainer.load_checkpoint">
<code class="descname">load_checkpoint</code><span class="sig-paren">(</span><em>checkpoint_path: str</em>, <em>iteration: Optional[int] = None</em><span class="sig-paren">)</span><a class="reference external" href="http://github.com/kdexd/probnmn-clevr/blob/master/probnmn/trainers/_trainer.py#L251-L274"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#probnmn.trainers._trainer._Trainer.load_checkpoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a checkpoint to continue training from. The iteration when this checkpoint was
serialized, is inferred from its name (so do not rename after serialization).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>checkpoint_path: str</strong></dt><dd><p>Path to a checkpoint containing models and optimizers of the phase which is being
trained on.</p>
</dd>
<dt><strong>iteration: int, optional (default = None)</strong></dt><dd><p>Iteration number. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, infer from name of checkpoint file.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="probnmn.trainers._trainer._Trainer._cycle">
<code class="descname">_cycle</code><span class="sig-paren">(</span><em>dataloader: torch.utils.data.dataloader.DataLoader</em><span class="sig-paren">)</span> &#x2192; Generator[[Dict[str, torch.Tensor], None], None]<a class="reference external" href="http://github.com/kdexd/probnmn-clevr/blob/master/probnmn/trainers/_trainer.py#L276-L291"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#probnmn.trainers._trainer._Trainer._cycle" title="Permalink to this definition">¶</a></dt>
<dd><p>A generator which yields a random batch from dataloader perpetually. This generator is
used in the constructor.</p>
<p>This is done so because we train for a fixed number of iterations, and do not have the
notion of ‘epochs’. Using <code class="docutils literal notranslate"><span class="pre">itertools.cycle</span></code> with dataloader is harmful and may cause
unexpeced memory leaks.</p>
</dd></dl>

</dd></dl>

</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">ProbNMN</a></h1>








<h3>Navigation</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="usage/setup_dependencies.html">How to setup this codebase?</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage/training.html">How to train your ProbNMN?</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage/evaluation_inference.html">How to evaluate or do inference?</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="config.html">probnmn.config</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">probnmn.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="models.html">probnmn.models</a></li>
<li class="toctree-l1"><a class="reference internal" href="modules.html">probnmn.modules</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="trainers.html">probnmn.trainers</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">probnmn.trainers._trainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="trainers.program_prior_trainer.html">probnmn.trainers.program_prior_trainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="trainers.question_coding_trainer.html">probnmn.trainers.question_coding_trainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="trainers.module_training_trainer.html">probnmn.trainers.module_training_trainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="trainers.joint_training_trainer.html">probnmn.trainers.joint_training_trainer</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="evaluators.html">probnmn.evaluators</a></li>
<li class="toctree-l1"><a class="reference internal" href="utils.html">probnmn.utils</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  <li><a href="trainers.html">probnmn.trainers</a><ul>
      <li>Previous: <a href="trainers.html" title="previous chapter">probnmn.trainers</a></li>
      <li>Next: <a href="trainers.program_prior_trainer.html" title="next chapter">probnmn.trainers.program_prior_trainer</a></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2019, Karan Desai.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 2.0.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/probnmn/trainers._trainer.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>